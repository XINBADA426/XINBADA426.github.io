<search>
    
     <entry>
        <title>Restricted cubic spline（RCS）：限制性立方样条</title>
        <url>https://xinbada426.github.io/post/2021-06-25/</url>
        <categories>
          <category>bioinformation</category>
        </categories>
        <tags>
          <tag>RCS</tag>
        </tags>
        <content type="html"> 当我们的自变量和因变量之间不是简单的线性关系，我们还可以通过多项式回归，多元线性回归等方法构造非线性的关系模型，限制性立方样条（RCS）也是一种选择。
什么是Restricted cubic spline 实在是不了解这个东西到底是怎么翻译成的，因为仅仅从他的中文译名来看限制性立方样条，我们可能会有这样的疑惑：这里的每个汉字我都认识，但连在一起到底是个什么玩意儿？
要了解什么是Restricted cubic spline（限制性立方样条），就要先了解什么是spline（样条函数）。
 样条函数 在计算机科学的计算机辅助设计和计算机图形学中，样条通常是指分段定义的多项式参数曲线。由于样条构造简单，使用方便，拟合准确，并能近似曲线拟合和交互式曲线设计中复杂的形状，样条是这些领域中曲线的常用表示方法。
 样条函数里面的关键词分段，然后再加上限制性立方样条中的关键词限制，是我们理解RCS的关键所在。同一般的非线性回归相比，限制性立方样条是分段的，每一区间有其自己的函数，且最两侧的区间都为线性函数。
示例代码 下面是一个使用Cox模型进行限制性立方样条分析的脚本，你可以将里面的模型替换为自己想用的模型~
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119  options(warn = -1) suppressPackageStartupMessages({ library(optparse) library(stringr) library(ggplot2) library(rms) library(logging) }) basicConfig() option_list &amp;lt;- list( make_option(c(&amp;#34;-t&amp;#34;, &amp;#34;--table&amp;#34;), type = &amp;#34;character&amp;#34;, help = &amp;#34;The input table&amp;#34;), make_option(c(&amp;#34;--time&amp;#34;), type = &amp;#34;character&amp;#34;, help = &amp;#34;The time tag&amp;#34;), make_option(c(&amp;#34;--dv&amp;#34;), type = &amp;#34;character&amp;#34;, help = &amp;#34;The dependent variable tag name&amp;#34;), make_option(c(&amp;#34;--iv&amp;#34;), type = &amp;#34;character&amp;#34;, help = &amp;#34;The Independent variable tag name(x)&amp;#34;), make_option(c(&amp;#34;--covariate&amp;#34;), type = &amp;#34;character&amp;#34;, help = &amp;#34;The covariate variable tag names(comma split)&amp;#34;), make_option( c(&amp;#34;--knots&amp;#34;), default = 3, type = &amp;#34;integer&amp;#34;, help = &amp;#34;The number of knots[default= %default]&amp;#34; ), make_option( c(&amp;#34;--prefix&amp;#34;), default = &amp;#34;res&amp;#34;, type = &amp;#34;character&amp;#34;, help = &amp;#34;The out put prefix[default= %default]&amp;#34; ) ) opts &amp;lt;- parse_args( OptionParser(usage = &amp;#34;%prog [options]&amp;#34;, option_list = option_list, description = &amp;#34;\n使用Cox比例风险回归模型进行限制性立方样条（Restricted cubic spline）分析&amp;#34;), positional_arguments = F ) #### Some Functions #### RCS_plot &amp;lt;- function(data, x) { p &amp;lt;- ggplot() &#43; geom_line( data = data, aes_string(x, &amp;#34;yhat&amp;#34;), linetype = &amp;#34;solid&amp;#34;, size = 1, alpha = 0.7, color = &amp;#34;#B521F5&amp;#34; ) &#43; geom_line( data = data, aes_string(x, &amp;#34;lower&amp;#34;), linetype = &amp;#34;dashed&amp;#34;, size = 1, alpha = 0.7, color = &amp;#34;#B521F5&amp;#34; ) &#43; geom_line( data = data, aes_string(x, &amp;#34;upper&amp;#34;), linetype = &amp;#34;dashed&amp;#34;, size = 1, alpha = 0.7, color = &amp;#34;#B521F5&amp;#34; ) &#43; geom_hline( yintercept = 1, linetype = &amp;#34;solid&amp;#34;, size = 0.5, color = &amp;#34;black&amp;#34; ) &#43; ylab(&amp;#34;HR(95%CI)\n&amp;#34;) &#43; xlab(str_c(&amp;#34;\n&amp;#34;, x)) &#43; theme_classic() &#43; theme(text = element_text(size = 20)) p } ######################## loginfo(&amp;#34;Read the input table&amp;#34;) data &amp;lt;- read.csv(opts$table, sep = &amp;#34;\t&amp;#34;, check.names = F) # Prepare loginfo(&amp;#34;Prepare the data&amp;#34;) dd &amp;lt;- datadist(data) options(datadist = &amp;#39;dd&amp;#39;) # Model loginfo(&amp;#34;Fit the model&amp;#34;) y &amp;lt;- str_c(&amp;#34;Surv(&amp;#34;, opts$time, &amp;#34;,&amp;#34;, opts$dv, &amp;#34;)&amp;#34;) x &amp;lt;- str_c(&amp;#34;rcs(&amp;#34;, opts$iv, &amp;#39;,&amp;#39;, opts$knots, &amp;#39;)&amp;#39;) if (!is.null(opts$covariate)) { list_cv &amp;lt;- str_split(opts$covariate, pattern = &amp;#39;,&amp;#39;)[[1]] cv &amp;lt;- paste(list_cv, sep = &amp;#39;,&amp;#39;) x &amp;lt;- paste(c(x, cv), sep = &amp;#34;,&amp;#34;) } form &amp;lt;- as.formula(paste(y, &amp;#34;~&amp;#34;, x)) fit &amp;lt;- cph(form, data = data) # Predict loginfo(&amp;#34;Predict&amp;#34;) HR &amp;lt;- Predict(fit, WBC, fun = exp, ref.zero = TRUE) # Plot loginfo(&amp;#34;Plot&amp;#34;) p &amp;lt;- RCS_plot(HR, opts$iv) ggsave( str_c(opts$prefix, &amp;#34;pdf&amp;#34;, sep = &amp;#39;.&amp;#39;), plot = p, width = 8, height = 7 )   名词解释 hazard ratio  hazard ratio 风险比率，正式的英文名称是Hazard Ratio。风险比率是两个风险率（Hazard Rates）的比值。风险率是单位时间内发生的事件数占被试总体的百分比。
 </content>
    </entry>
    
     <entry>
        <title>MetaStat原理</title>
        <url>https://xinbada426.github.io/post/2021-06-01/</url>
        <categories>
          <category>bioinformation</category>
        </categories>
        <tags>
          <tag>宏基因组</tag>
        </tags>
        <content type="html"> 一直不清楚MetaStat的脚本输入到底应该是Read数还是相对丰度，大概过了一下R语言版本的MetaStat代码的脚本，留此记录。
代码解析 R语言版本的MetaStat被封装到了一个R函数内部，针对不同的数据输入，MetaStat会采用不同的处理策略
两个比较组都是单样本无重复 此时MetatStat使用Fisher精确检验或卡方检验（每个样本内tag或reads数都大于20） 涉及到Fisher或卡方检验的输入都是列联表，肯定都是整数因此理想的输入的表格内都应该是整数
有重复 MetaStat对于有重复的样本（或者说是上述情况以外）的比较组的统计分为两步
 对于输入的每一行，都会通过T检验计算其t统计量（这里使用的是每个样本的相对丰度），然后通过t统计量利用置换检验算出p值 重新遍历每一行，对于在两个比较组内平均tag数小于1的行，重新利用Fisher精确检验计算p值替换前一步得到的该物种的p值  感想 综上所述，应该使用count/reads值作为其函数的输入
吐槽  这个R代码内各种for循环~ 效率真是不高~ 有空看一下置换检验，这里置换检验也是限制代码执行时间的坑~ 好像mothur里面有C&#43;&#43;的版本，有时间试一下  </content>
    </entry>
    
     <entry>
        <title>Centos7 中各种国内源的配置</title>
        <url>https://xinbada426.github.io/post/2020-08-23/</url>
        <categories>
          <category>Linux</category>
        </categories>
        <tags>
          <tag>Centos7</tag><tag>镜像</tag>
        </tags>
        <content type="html">  由于国内网络环境的原因，CentOS7 下各种软件更新、安装速度可能会非常非常非常慢，好在一般都有国内的镜像源可以替代，这里记录我常用的各种源的配置，便于以后查找。
 yum 源 yum 源的配置当然需要 root 权限啦~请做好备份
 修改 /etc/yum/pluginconf.d/fastestmirror.conf 内 enabled=0, 关闭 yum-fastestmirror 插件（如果有的话） 修改配置文件，清华大学 yum 源网页上的说明已经超级详细了，按照操作来就好。  Anaconda 源 Anaconda 镜像使用帮助
 修改主目录下的 .condarc 文件（channel_alias 同网站的不一样）  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  channels: - defaults show_channel_urls: true channel_alias: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud   conda clean -i 清除索引  </content>
    </entry>
    
     <entry>
        <title>如何将表格文件转为 sqlite3 数据库文件</title>
        <url>https://xinbada426.github.io/post/2020-08-18/</url>
        <categories>
          <category>Others</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>SQL</tag>
        </tags>
        <content type="html">  平时工作中碰到的需求，需要将 tsv/csv 文件转为 sqlite3 数据库文件，本来一直想用 python 的数据库接口来实现这一目的，但经在网上的搜索，发现直接使用 sqlite3 的点命令更为方便，特此记录。
 数据准备 1  for i in {1..10};do echo -e &amp;#34;$i\t${i}_${i}&amp;#34;;done &amp;gt; test.tsv   数据库生成  连接数据库  1 2  # 如果没有 test.db, 则会自动生成 sqlite3 test.db   使用 sqlite3 的点命令生成表  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # 查看数据库内已有的 table .tables # 设定为制表符输出 .mode tabs # 有头，首行将作为 headr，把表格导入名为 head 的 table 中 .import test.tsv head # 查看 header .schema head # 查看前5行 SELECT * FROM head LIMIT 5; # 2	2_2 # 3	3_3 # 4	4_4 # 5	5_5 # 6	6_6 # 无头，把表格导入名为 nohead 的 table 中 CREATE TABLE nohead(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;); .import test.tsv nohead # 查看 header .schema nohead # 查看前5行 SELECT * FROM nohead LIMIT 5; # 1	1_1 # 2	2_2 # 3	3_3 # 4	4_4 # 5	5_5 # 退出数据库 .quit   备注 常用的点命令可使用 .help 查询，.help command可查看某一点命令的具体使用方法
</content>
    </entry>
    
     <entry>
        <title>HUGO 备忘</title>
        <url>https://xinbada426.github.io/post/2020-08-17/</url>
        <categories>
          <category>Others</category>
        </categories>
        <tags>
          <tag>HUGO</tag>
        </tags>
        <content type="html">  这是一个 HUGO 使用的备忘
 常用命令 新建文章 hugo new post/file_name.md 如要发布，记得修改 draft 为 true
在本地查看 hugo serve -D
建立静态网页 1 2 3 4  # 不编译 draft 为 true 的文章 hugo # 编译 draft 为 true 的文章 hugo -D   发布  进入 public 目录 使用 git 推送到 github  </content>
    </entry>
    
     <entry>
        <title>Blast2GO 本地化</title>
        <url>https://xinbada426.github.io/post/2020-07-23/</url>
        <categories>
          <category>bioinformation</category>
        </categories>
        <tags>
          <tag>Blast2GO</tag><tag>数据库</tag>
        </tags>
        <content type="html"> 准备 Linux版本信息 1 2  #&amp;gt; cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core)   mysql 如果你从未使用过mysql
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # 下载源 #&amp;gt; wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm # 安装源 #&amp;gt; rpm -ivh mysql-community-release-el7-5.noarch.rpm # 安装 #&amp;gt; yum install -y mysql-server # 查看版本信息 #&amp;gt; mysql -V mysql Ver 14.14 Distrib 5.6.49, for Linux (x86_64) using EditLine wrapper # 启动mysql服务 #&amp;gt; systemctl start mysqld # 设定mysql root密码 #&amp;gt; /usr/bin/mysqladmin -u root password &amp;#39;your password&amp;#39; # 查看状态 #&amp;gt; systemctl status mysqld # 设定开机启动 #&amp;gt; systemctl enable mysqld   Java 没有使用网上推荐的 JDK 6，7，各个步骤正常
1 2 3 4  $&amp;gt; java -version openjdk version &amp;#34;1.8.0_242&amp;#34; OpenJDK Runtime Environment (build 1.8.0_242-b08) OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)   Data  gene2accession.gz gene_info.gz idmapping.tb.gz godatabase  b2g4pipe b2g4pipe_v2.5.zip 提取码：6p3s
local_b2g_db local_b2g_db.tar.bz2 提取码：drb9
数据库安装 创建b2g数据库并初始化各个表 1 2 3 4 5 6 7 8 9 10 11 12  # 创建 `gene2accession` `gene_info` `gi2uniprot` 这三个表 #&amp;gt; mysql -uroot -p &amp;lt; b2gdb.sql # 利用 go_monthly-assocdb 创建其它的表 # `assoc_rel` `association` `association_isoform` `association_property` `association_qualifier` `association_species_qualifier` `db` `dbxref` `evidence` `evidence_dbxref` `gene_product` `gene_product_ancestor` `gene_product_count` `gene_product_dbxref` `gene_product_homology` `gene_product_homolset` `gene_product_phylotree` `gene_product_property` `gene_product_seq` `gene_product_subset` `gene_product_synonym` `graph_path` `graph_path2term` `homolset` `instance_data` `intersection_of` `phylotree` `phylotree_property` `relation_composition``relation_properties` `seq` `seq_dbxref` `seq_property` `source_audit` `species` `term` `term2term` `term2term_metadata` `term_audit` `term_dbxref` `term_definition` `term_property` `term_subset` `term_synonym` #&amp;gt; mysql -uroot -p b2g &amp;lt; go_monthly-assocdb-data # 将文件 gene2accession 导入 表 `gene2accession` #&amp;gt; mysql -uroot -p b2g -e&amp;#34;LOAD DATA LOCAL INFILE &amp;#39;gene2accession&amp;#39; INTO TABLE gene2accession FIELDS TERMINATED BY &amp;#39;\t&amp;#39; LINES TERMINATED BY &amp;#39;\n&amp;#39;;&amp;#34; # 导入各种id映射关系 #&amp;gt; java -cp .:mysql-connector-java-5.0.8-bin.jar: ImportIdMapping idmapping.tb localhost b2g blast2go blast4it   示例运行  解压 b2g4pipe-2.5.0 并进入文件夹 修改 b2gPipe.properties 内的数据库信息如下（同在 b2gdb.sql 的构建数据库时的信息）  1 2 3 4 5  // GO and B2G Data Access Basic Dbacces.dbname=b2g Dbacces.dbhost=localhost Dbacces.dbuser=blast2go Dbacces.dbpasswd=blast4it   运行目录下的 runPipeExample.sh  已知问题  go_monthly-assocdb-data.gz这个文件最后的更新是2017年1月 数据库太大，我用来测试的破机子硬盘还是有点压力的  参考  本地Blast2GO安装，及其数据库更新和导入数据中断的解决方案-  </content>
    </entry>
    
     <entry>
        <title>PBS作业投递系统使用</title>
        <url>https://xinbada426.github.io/post/2020-06-01/</url>
        <categories>
          <category>Others</category>
        </categories>
        <tags>
          <tag>PBS</tag>
        </tags>
        <content type="html"> 系统架构  计算节点共有5台曙光I840-G30四路服务器。每台服务器配置4颗20核Intel Xeon 6248 2.5GHz处理器，1536G DDR4内存。 GPU计算节点为1台曙光X745-G30双路服务器。配置2颗20核Intel Xeon 6230 2.1GHz处理器，256G DDR3内存，4张GeForce RTX 2080 Ti显卡。 存储系统采用lustre文件系统，使用一台曙光I620-G30作为元数据服务器，两台曙光S640-G30作为数据服务器，系统提供260T可用空间，为集群系统提供高效、稳定读写性能和存储资源的灵活快速的部署能力。 系统配置管理节点1台，型号为曙光I620-G30双路服务器，部署曙光Gridview 4.0集群管理系统。用户可通过IP地址实现Web访问。 系统配置登录节点1台，型号为曙光I620-G30双路服务器，用户可从内网直接登录该节点实现作业提交、编译程序等。 系统配置1套互联的万兆光纤以太网作为管理网络，配置1套100Gb Infiniband高速网络作为计算网路。 硬件整体架构采用冗余设备或部件，保证设备、部件、链路的高可用性及数据读写、传输的稳定可靠进。  作业调度常用命令 获取所有的节点信息（qhost） /opt/gridview/pbs/dispatcher/bin/lsf_cmd/listhosts
获取所有队列信息（qqueue） /opt/gridview/pbs/dispatcher/bin/lsf_cmd/listqueues
获取所有任务信息(qa) /opt/gridview/pbs/dispatcher/bin/lsf_cmd/listjobs
获取我的任务信息（qs） /opt/gridview/pbs/dispatcher/bin/qstat
任务投递（qsub） /opt/gridview/pbs/dispatcher/bin/qsub
1 2 3 4  # 投递任务到指定节点 qsub -l nodes=node3 t.sh # 投递任务到指定队列 qsub -q middle t.sh   常用目录 管理员软件安装目录 /public/software/
我的软件安装目录 /public/home/guest/softwares
PBS安装目录 /opt/gridview/pbs/dispatcher/bin
</content>
    </entry>
    
     <entry>
        <title>CAZy数据库简介</title>
        <url>https://xinbada426.github.io/post/2018-02-06/</url>
        <categories>
          <category>bioinformation</category>
        </categories>
        <tags>
          <tag>数据库</tag><tag>CAZy</tag>
        </tags>
        <content type="html"> 碳水化合物活性酶数据库(CAZy) 提供了酶分子序列的家族信息，物种来源，基因序列，蛋白序列，三维结构，EC分类，相关数据库链接…… 建立此数据库是为了将酶分子的序列、结构、催化机制相关联~
分类 通过序列相似性进行家族分类，在这6个大的分类下可继续分为更为细致的亚类。
   名称 缩写     糖苷水解酶类 GHs   糖苷转移酶类 GTs   多糖裂解酶类 PLs   糖水化合物脂酶类 CEs   碳水化合物结合模块 CBMs   辅助模块酶类 AAs    名称说明 使用 CAZy的官网只提供了web查找的方法，如果需要本地化，需要自行爬取数据或者使用dbCAN的数据.
下载数据的说明
</content>
    </entry>
    
     <entry>
        <title>GTF/GFF文件的差异及其相互转换</title>
        <url>https://xinbada426.github.io/post/2016-09-08/</url>
        <categories>
          <category>bioinformation</category>
        </categories>
        <tags>
          <tag>gtf</tag><tag>gff</tag>
        </tags>
        <content type="html"> 我们在做生物分析的时候，经常会碰到GFF格式的文件以及GTF格式的注释文件。他们有着相似的名字，甚至连内容都极为相似~那么，他们究竟差在哪里呢？
GFF全称为general feature format，这种格式主要是用来注释基因组。 GTF全称为gene transfer format，主要是用来对基因进行注释。
 数据结构 GTF文件以及GFF文件都由9列数据组成，这两种文件的前8列都是相同的（一些小的差别）
   列 内容     1 reference sequence name   2 annotation source   3 feature type   4 start coordinate   5 end coordinate   6 score   7 strand   8 frame   9 attributes    GFF GFF文件是一种用来描述基因组特征的文件，现在我们所使用的大部分都是第三版）（GFF3）。GFF允许使用#作为注释符号，例如很多GFF文件都会使用如下的两行来表明其版本其创建日期：
1 2  ##gff-version 2 ##created 11/11/11   GFF文件每一列所代表的含义前面表格中有，但请注意，它的第3列feature type是不受约束的，你可以使用任意的名称，但也不要太淘气~用一些适当的名称对于后面的分析会有很大的帮助。 我们需要注意的是GFF文件的第9列，从第二版开始(GFF2)，所有的属性都以标签=值的方式呈现，各个属性之间以;作为分隔符
1  ID=geneAExon1;Name=geneA;Parent=geneA;Organism=human   在最新版本的GFF文件中(GFF3)，有一些是已经预先定义的属性特征，并且这些特征往往还有特殊的含义：ID这个标签实在各行都要有的；另外有一个Parent的属性，它表明了当前的特征是Parent特征的子集。
1 2 3  Contig01 PFAM gene 501 750 . &#43; 0 ID=geneA;Name=geneA Contig01 PFAM exon 501 650 . &#43; 2 ID=exonA1;Parent=geneA Contig01 PFAM exon 700 750 . &#43; 2 ID=exonA2;Parent=geneA   GTF 当前所广泛使用的GTF格式为第二版(GTF2)，它主要是用来描述基因的注释。GTF格式有两个硬性标准：
 根据所使用的软件的不同，feature types是必须注明的。 第9列必须以gene_id以及transcript_id开头 GTF文件的第9列同GFF文件不同，虽然同样是标签与值配对的情况，但标签与值之间以空格分开，且每个特征之后都要有分号;（包括最后一个特征）：  1  gene_id &amp;#34;geneA&amp;#34;;transcript_id &amp;#34;geneA.1&amp;#34;;database_id &amp;#34;0012&amp;#34;;modified_by &amp;#34;Damian&amp;#34;;duplicates 0;   两种文件的差异比较    列 GTF2 GFF3     reference sequence name same same   annotation source same same   feature type feature requirements depend on software can be anything   start coordinate same same   5. end coordinate same same   score not used optional   strand same same   frame same same   attributes 空格分隔 =分隔    两种文件格式之间的转换 偷个懒，直接使用Cufflinks里面的工具gffread
1 2 3 4  #gff2gtf gffread my.gff3 -T -o my.gtf #gtf2gff gffread merged.gtf -o- &amp;gt; merged.gff3    参考（可能需要梯子）
</content>
    </entry>
    
</search>
